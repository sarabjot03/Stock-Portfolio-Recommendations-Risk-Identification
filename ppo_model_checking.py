# -*- coding: utf-8 -*-
"""PPO_Model_Checking.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19r4cA9pBv1_pegG0mcF-_Uhbgjb1D1dU
"""

import pandas as pd
import numpy as np
import joblib
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import PPO
from stable_baselines3.common.env_checker import check_env
import matplotlib.pyplot as plt

# Step 1: Load Dataset and Model
data = pd.read_csv("/Users/sarabjotsingh/Downloads/Feature_engineering.csv")
rf_model = joblib.load('/Users/sarabjotsingh/Downloads/best_rf_model.pkl')

# Step 2: Feature Selection
selected_features = [
    'up', 'prev_diff', 'daydiff', 'low', 'capital gains',
    'high', 'prev_close', 'open', 'adj close'
]
lags = [1, 2, 3]

# Step 3: Create Lagged Features
for feature in ['up', 'prev_diff', 'daydiff']:
    for lag in lags:
        data[f'{feature}_lag{lag}'] = data[feature].shift(lag)
data.dropna(inplace=True)  # Remove rows with NaN from lagging

# Step 4: Predict RSI with Pre-Trained Random Forest Model
lagged_features = [f'{feature}_lag{lag}' for feature in ['up', 'prev_diff', 'daydiff'] for lag in lags]
X_for_rsi = data[selected_features + lagged_features]
data['predicted_rsi'] = rf_model.predict(X_for_rsi)

# Step 5: Scale Features
scaler = StandardScaler()
scaled_features = scaler.fit_transform(X_for_rsi)
data_scaled = pd.DataFrame(scaled_features, columns=X_for_rsi.columns)
data_scaled[['symbol', 'date']] = data[['symbol', 'date']].reset_index(drop=True)

# Step 6: Train-Test Split
grouped = data.groupby('symbol')
train_groups, test_groups = [], []
for symbol, group in grouped:
    split_idx = int(len(group) * 0.7)
    train_groups.append(group.iloc[:split_idx])
    test_groups.append(group.iloc[split_idx:])
train_data = pd.concat(train_groups)
test_data = pd.concat(test_groups)

# Ensure common symbols between train and test
common_symbols = set(train_data['symbol']).intersection(test_data['symbol'])
train_data = train_data[train_data['symbol'].isin(common_symbols)]
test_data = test_data[test_data['symbol'].isin(common_symbols)]

# Scale Train and Test Data Separately
X_train = scaler.transform(train_data[selected_features + lagged_features])
X_test = scaler.transform(test_data[selected_features + lagged_features])

train_data_scaled = train_data.copy()
test_data_scaled = test_data.copy()
train_data_scaled[selected_features + lagged_features] = X_train
test_data_scaled[selected_features + lagged_features] = X_test

# Step 7: Define Reinforcement Learning Environment
class StockPortfolioEnv(gym.Env):
    def __init__(self, data, budget, risk_tolerance, stock_count=6):
        super().__init__()
        self.data = data.reset_index(drop=True)
        self.budget = budget
        self.risk_tolerance = risk_tolerance
        self.stock_count = stock_count

        self.stock_symbols = self.data['symbol'].unique()
        self.num_stocks = len(self.stock_symbols)
        assert self.num_stocks >= stock_count, "Not enough unique stocks in data."

        self.action_space = spaces.MultiDiscrete([self.num_stocks] * stock_count)
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(self.data.shape[1] - 2,), dtype=np.float32
        )

        self.current_step = 0

    def reset(self, seed=None, options=None):
        if seed is not None:
            np.random.seed(seed)
        self.current_step = 0
        return self._get_observation(), {}

    def step(self, action):
        selected_stocks = [self.stock_symbols[idx] for idx in action]
        reward, cost = self._compute_reward(selected_stocks)
        self.current_step += 1
        done = self.current_step >= len(self.data) - 1
        return self._get_observation(), reward, done, False, {"selected_stocks": selected_stocks, "portfolio_cost": cost}

    def _get_observation(self):
        return self.data.iloc[self.current_step, :].drop(['symbol', 'date']).values.astype(np.float32)

    def _compute_reward(self, selected_stocks):
        selected_data = self.data[self.data['symbol'].isin(selected_stocks)]
        selected_data['price_return'] = (selected_data['adj close'] - selected_data['prev_close']) / selected_data['prev_close']
        portfolio_return = selected_data['price_return'].mean() + selected_data['capital gains'].mean()

        selected_data['price_volatility'] = selected_data['adj close'].rolling(14).std()
        portfolio_risk = selected_data['price_volatility'].mean()

        risk_weight = {"low": 2.0, "medium": 1.0, "high": 0.5}[self.risk_tolerance]
        reward = portfolio_return - risk_weight * portfolio_risk

        portfolio_cost = selected_data['adj close'].sum()
        if portfolio_cost > self.budget:
            reward -= 100
        return reward, portfolio_cost

# Step 8: Train PPO Model
env = StockPortfolioEnv(data=train_data_scaled, budget=10000, risk_tolerance="medium", stock_count=6)
check_env(env, warn=True)
model = PPO("MlpPolicy", env, verbose=1)
model.learn(total_timesteps=2000)

# Update test environment with reduced dataset size
filtered_test_data = test_data_scaled[
    (test_data_scaled['date'] >= "2024-09-01") & (test_data_scaled['date'] <= "2024-09-30")
]
test_env = StockPortfolioEnv(data=filtered_test_data, budget=10000, risk_tolerance="medium", stock_count=6)

# Pre-compute any static values
filtered_test_data['price_volatility'] = (
    filtered_test_data['adj close']
    .rolling(window=14)
    .std()
)

# Reset environment
state, _ = test_env.reset()
done, best_reward, best_portfolio = False, float('-inf'), None

# Step loop with optimizations
while not done:
    action, _ = model.predict(state)
    valid_action = [min(idx, len(test_env.stock_symbols) - 1) for idx in action]
    state, reward, done, _, info = test_env.step(valid_action)

    # Track best portfolio
    if reward > best_reward:
        best_reward = reward
        best_portfolio = info["selected_stocks"]

print(f"Best Portfolio: {best_portfolio}, Reward: {best_reward}")

# Plot Portfolio Performance
portfolio_data = filtered_test_data[filtered_test_data['symbol'].isin(best_portfolio)]
portfolio_data['portfolio_value'] = portfolio_data.groupby('date')['adj close'].transform('sum')
plt.figure(figsize=(10, 6))
plt.plot(portfolio_data['date'], portfolio_data['portfolio_value'], label="Portfolio Value")
plt.xlabel("Date")
plt.ylabel("Portfolio Value")
plt.title(f"Performance of Portfolio: {', '.join(best_portfolio)}")
plt.legend()
plt.grid()
plt.show()

# Filter data for the specific stocks and date range
selected_stocks = ['LIT', 'EMR', 'GGM', 'ANEW', 'WELL', 'IBHD']
filtered_data = data[data['symbol'].isin(selected_stocks)]

# Parse the date column and filter the date range
filtered_data['date'] = pd.to_datetime(filtered_data['date'])
date_range_data = filtered_data[
    (filtered_data['date'] >= '2024-09-02') & (filtered_data['date'] <= '2024-09-05')
]

# Sort data by symbol and date for further calculations
date_range_data = date_range_data.sort_values(by=['symbol', 'date'])

# Inspect the filtered dataset for the portfolio
date_range_data.head()

# Access data from the environment
# Calculate portfolio cost directly from the environment data
filtered_test_data['portfolio_cost'] = filtered_test_data.groupby('date')['adj close'].sum()

# Count budget violations
budget_violations = (filtered_test_data['portfolio_cost'] > test_env.budget).sum()
print(f"Budget Violations: {budget_violations}")